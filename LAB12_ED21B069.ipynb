{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12 : Neural Networks Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Implement the two layer network for m-samples, n-features as we discussed in class (both FP and BP) and for N layers in the hidden layer. Split the data (you can use the log. reg. data or any other one) and train your network with 80% of the data. Test your network with the test data. Report the evaluation metrics for varying number of layers in the network. Also evaluate one more activation function (ReLU/tanh) other than sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"Logistic_regression_ls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n",
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "# this data set includes 3 features \n",
    "n=int(input(\"Enter how many features you want to use: \"))\n",
    "h=int(input(\"Enter the number of neurons in the hidden layer\"))\n",
    "\n",
    "dat=np.array(df)\n",
    "X=dat[:,:n]\n",
    "y=dat[:,-1:]\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
    "r,c=np.shape(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    z= (1/(1+np.exp(-x)))\n",
    "    return z\n",
    "def init_params():\n",
    "    W1 = np.random.randn(h, n) * 0.01\n",
    "    b1 = np.zeros((h, 1))\n",
    "    W2 = np.random.randn(1, h) * 0.01\n",
    "    b2 = np.zeros((1, 1))\n",
    "    return W1, b1, W2, b2\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X.T) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def forward_prop_ReLU(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X.T) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = ReLU(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(Z1):\n",
    "   return np.multiply(sigmoid(Z1),(1-sigmoid(Z1)))\n",
    "def ReLU_deriv(Z1):\n",
    "    return np.greater(Z1, 0).astype(int)\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m=Y.size\n",
    "    dZ2 = A2 - Y.T\n",
    "    dW2 = (1 / m) * dZ2.dot(A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.multiply(W2.T.dot(dZ2) , (sigmoid_deriv(Z1)))\n",
    "    dW1 = (1 / m) * dZ1.dot(X)\n",
    "    db1 = (1 / m) * np.sum(dZ1,axis=1,keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def backward_prop_ReLU(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m=Y.size\n",
    "    dZ2 = A2 - Y.T\n",
    "    dW2 = (1 / m) * dZ2.dot(A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.multiply(W2.T.dot(dZ2) , (ReLU_deriv(Z1)))\n",
    "    dW1 = (1 / m) * dZ1.dot(X)\n",
    "    db1 = (1 / m) * np.sum(dZ1,axis=1,keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        loss = -np.mean(Y * np.log(A2.T) + (1 - Y) * np.log(1 - A2.T))\n",
    "    print(loss)\n",
    "    return W1, b1, W2, b2\n",
    "def gradient_descent_ReLU(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop_ReLU(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop_ReLU(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        loss = -np.mean(Y * np.log(A2.T) + (1 - Y) * np.log(1 - A2.T))\n",
    "    print(loss)\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0796479020328726\n",
      "nan\n",
      "[[ 0.31187081  0.31193197]\n",
      " [ 0.27370771  0.27842808]\n",
      " [-0.46616253 -0.44441808]\n",
      " [ 0.24126922  0.2343634 ]]\n",
      "[[ 2.27716023  1.85290578 -4.57624754  1.35431055]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shobhith Vadlamudi\\AppData\\Local\\Temp\\ipykernel_30576\\4039663226.py:47: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(Y * np.log(A2.T) + (1 - Y) * np.log(1 - A2.T))\n",
      "C:\\Users\\Shobhith Vadlamudi\\AppData\\Local\\Temp\\ipykernel_30576\\4039663226.py:47: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.mean(Y * np.log(A2.T) + (1 - Y) * np.log(1 - A2.T))\n",
      "C:\\Users\\Shobhith Vadlamudi\\AppData\\Local\\Temp\\ipykernel_30576\\4039663226.py:47: RuntimeWarning: invalid value encountered in log\n",
      "  loss = -np.mean(Y * np.log(A2.T) + (1 - Y) * np.log(1 - A2.T))\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, y_train, 0.1, 1000)\n",
    "W_1, b_1, W_2, b_2 = gradient_descent_ReLU(X_train, y_train, 0.1, 1000)\n",
    "print(W1)\n",
    "print(W2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the neural network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1, A1, Z2, A2= forward_prop(W1,b1,W2,b2,X_test)\n",
    "Z_1,A_1,Z_2,A_2=forward_prop_ReLU(W_1, b_1, W_2, b_2,X_test)\n",
    "A2=np.round(A2,0)\n",
    "A_2=np.round(A_2,0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scoere=  1.0\n",
      "Precision score=  1.0\n",
      "Recall score=  1.0\n",
      "f1 score=  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Accuracy scoere= \",accuracy_score(y_test, A2.T))\n",
    "print(\"Precision score= \",precision_score(y_test, A2.T))\n",
    "print(\"Recall score= \",recall_score(y_test, A2.T))\n",
    "print(\"f1 score= \",f1_score(y_test, A2.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scoere=  1.0\n",
      "Precision score=  1.0\n",
      "Recall score=  1.0\n",
      "f1 score=  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy scoere= \",accuracy_score(y_test, A_2.T))\n",
    "print(\"Precision score= \",precision_score(y_test, A_2.T))\n",
    "print(\"Recall score= \",recall_score(y_test, A_2.T))\n",
    "print(\"f1 score= \",f1_score(y_test, A_2.T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think there were too less samples in this data set for this neural network as we were able to achieve 100 percent accuracy very quickly even with less neurons in the hidden layer. Taking 1 feature and 1 neuron the metrics I was getting are \n",
    "- accuracy- 0.97\n",
    "- precision- 0.95\n",
    "- recall score- 0.97\n",
    "- f1 score- 0.96"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
